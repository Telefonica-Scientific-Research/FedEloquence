criterion:
  type: CrossEntropyLoss
data:
  root: data/
  splits:
  - 0.96
  - 0.02
  - 0.02
  splitter: iid
  type: xlsum_4c.jsonl@llm
  domain_type: cross
  shuffle: False
dataloader:
  batch_size: 1
device: 0
early_stop:
  patience: 0
eval:
  count_flops: false
  freq: 10
  metrics:
  - loss
  len_server_dataset: 2000
expname_tag: ds_4c_1000r_30ls
federate:
  client_num: 4
  master_port: 29340
  method: FedAvg
  mode: standalone
  online_aggr: false
  process_num: 1
  adapt_save_to: finetuned_adapters/multilingual/xlsum/4c/salamandra-2b-instruct/ds_4c_1000r_30ls.ckpt
  share_local_model: true
  total_round_num: 1000
  shuffle_trainining_data: False
llm:
  adapter:
    args:
    - adapter_method: lora
      adapter_package: peft
      lora_alpha: 16
      lora_dropout: 0.05
      r: 8
      target_modules:
      - q_proj
      - k_proj
      - v_proj
      - o_proj
      - gate_proj
      - up_proj
      - down_proj
      - lm_head
    use: true
  chat:
    max_len: 4000
  deepspeed:
    ds_config: federatedscope/llm/deepspeed/ds_config_4bs.json
    use: true
  tok_len: 3000
  prompt_path: prompt_templates/xlsum_4c
  model_save_to: models_with_ft_adapt/multilingual/xlsum/4c/salamandra-2b-instruct
  to_hf_format: 
    use: true
    hash_code_model_snapshot: ""
model:
  type: BSC-LT/salamandra-2b-instruct@huggingface_llm
train:
  batch_or_epoch: batch
  is_enable_half: true
  local_update_steps: 30
  optimizer:
    lr: 0.0003
    weight_decay: 0.0
trainer:
  type: llmtrainer
use_gpu: true