criterion:
  type: CrossEntropyLoss
data:
  root: data/
  splits:
  - 0.96
  - 0.02
  - 0.02
  splitter: iid
  type: en.jsonl@llm
  domain_type: cross
dataloader:
  batch_size: 3
device: 3
early_stop:
  patience: 3
  delta: 0.003
eval:
  count_flops: false
  freq: 1
  split:
  - val
  metrics:
  - loss
  len_server_dataset: 501
  best_res_update_round_wise_key: val_avg_loss
expname_tag: ds_8c_5000r_r16_a32_3pat_1eval_3b_16320lts_EN
federate:
  client_num: 1
  master_port: 29340
  method: global
  mode: standalone
  online_aggr: false
  process_num: 1
  adapt_save_to: finetuned_adapters/multilingual/alpaca_cleaned/8c/salamandra-2b-instruct/global/ds_8c_5000r_r16_a32_3pat_1eval_3b_16320lts_EN.ckpt
  share_local_model: true
  total_round_num: 5000
  use_global_early_stop: True
  use_LDES: False
  shuffle_all_data: False
  shuffle_train_clients: True
  shuffle_val_clients: True
  shuffle_test_clients: True
  make_global_eval: False
  make_clients_eval: True
llm:
  adapter:
    args:
    - adapter_method: lora
      adapter_package: peft
      lora_alpha: 32
      lora_dropout: 0.05
      r: 16
      target_modules:
      - q_proj
      - k_proj
      - v_proj
      - o_proj
      - gate_proj
      - up_proj
      - down_proj
      - lm_head
    use: true
  chat:
    max_len: 1500
  deepspeed:
    ds_config: federatedscope/llm/deepspeed/ds_config_192bs_16grad_3mbs.json
    use: true
  tok_len: 1300
  prompt_path: prompt_templates/alpaca_cleaned_8c
  model_save_to: models_with_ft_adapt/multilingual/alpaca_cleaned/8c/global/salamandra-2b-instruct
  to_hf_format: 
    use: false
    hash_code_model_snapshot: ""
model:
  type: BSC-LT/salamandra-2b-instruct@huggingface_llm
train:
  batch_or_epoch: batch
  is_enable_half: true
  local_update_steps: 16320
  optimizer:
    lr: 0.001
    weight_decay: 0.0
trainer:
  type: llmtrainer
use_gpu: true